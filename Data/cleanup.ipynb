{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook describes the cleanup process of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import string\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import langdetect\n",
    "import tqdm\n",
    "import itertools as it\n",
    "from operator import add\n",
    "\n",
    "from wordcloud import WordCloud\n",
    "from collections import defaultdict\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read data\n",
    "df = pd.read_csv(r\"C:\\Users\\anees\\Documents\\Work\\Twitter\\nicotine_filter.csv\", error_bad_lines=False, warn_bad_lines=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(434769, 4)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Certain cells are empty or corrupted. We'll have to remove them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(418416, 4)\n",
      "True (418206, 4)\n",
      "(418202, 4)\n",
      "(417139, 4)\n"
     ]
    }
   ],
   "source": [
    "df = df.dropna()\n",
    "print(df.shape)\n",
    "df = df[df['id'].map(len) == 19]\n",
    "print(df['id'].is_unique, df.shape)\n",
    "df = df[df['CreatedAt'].map(len) == 19] \n",
    "print(df.shape)\n",
    "df = df[df['userId'].map(str.isnumeric) == True]\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus, 417139 tweets contain the keyword 'nicotine'. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('cleaned.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_nonenglish(df):\n",
    "    '''\n",
    "        removes non-english tweets from the data\n",
    "    '''\n",
    "    en = 0\n",
    "    flags = []\n",
    "    for post in tqdm.tqdm(text): \n",
    "        try:\n",
    "            ch = langdetect.detect(post)\n",
    "            if ch == 'en':\n",
    "                en +=1\n",
    "                flags.append(True)\n",
    "            else:\n",
    "                flags.append(False)\n",
    "        except Exception as e:\n",
    "            if e == Exception.KeyboardInterrupt:\n",
    "                break\n",
    "            print(post)\n",
    "            flags.append(False)\n",
    "    return df[flags]\n",
    "\n",
    "stopwords = set(nltk.corpus.stopwords.words('english'))\n",
    "punctuation = (string.punctuation)\n",
    "url_regex = re.compile(\n",
    "    r'^(?:http|ftp)s?://' # http:// or https://\n",
    "    r'(?:(?:[A-Z0-9](?:[A-Z0-9-]{0,61}[A-Z0-9])?\\.)+(?:[A-Z]{2,6}\\.?|[A-Z0-9-]{2,}\\.?)|' #domain...\n",
    "    r'localhost|' #localhost...\n",
    "    r'\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\\.\\d{1,3})' # ...or ip\n",
    "    r'(?::\\d+)?' # optional port\n",
    "    r'(?:/?|[/?]\\S+)$', re.IGNORECASE)\n",
    "tokenizer = nltk.tokenize.TweetTokenizer()\n",
    "lemmatizer = nltk.WordNetLemmatizer()\n",
    "punctuations = set(string.punctuation)\n",
    "\n",
    "def lemmatizeToken(token, tag):\n",
    "    '''\n",
    "        Lemmatizer function. Four tags are considered: nouns, verbs, adjectives and adverbs\n",
    "    '''\n",
    "    tag = {\n",
    "        'N': nltk.corpus.wordnet.NOUN,\n",
    "        'V': nltk.corpus.wordnet.VERB,\n",
    "        'R': nltk.corpus.wordnet.ADV,\n",
    "        'J': nltk.corpus.wordnet.ADJ\n",
    "    }.get(tag[0], nltk.corpus.wordnet.NOUN)\n",
    "    return lemmatizer.lemmatize(token, tag)\n",
    "\n",
    "def normalize_tweets(tweets,lemmatize):\n",
    "    '''\n",
    "        Creates one-grams and bi-grams from text data. \n",
    "        lemmatize=1 lemmatizes the data.\n",
    "        Data is also normalized during the process. (removes hashtags, urls, stopwords\n",
    "        , basic punctuation, non-printable characters)\n",
    "        normalizes mentions\n",
    "    '''\n",
    "    onegrams = defaultdict(lambda:0)\n",
    "    bigrams = defaultdict(lambda:0)\n",
    "    onegram_dict = defaultdict(lambda:0)\n",
    "    bigram_dict = defaultdict(lambda:0)\n",
    "    loc_onegrams = []\n",
    "    loc_bigrams = []\n",
    "    prev = None\n",
    "    for i in tqdm.trange(len(tweets)):\n",
    "        tweet = tweets[i]\n",
    "        lonegram = set()\n",
    "        lbigram = set()\n",
    "        tweet = tweet.encode('ascii', 'ignore').decode('ascii')\n",
    "        for token, tag in nltk.pos_tag(tokenizer.tokenize(tweet.lower())):\n",
    "            if token[0] == '#' or len(token) < 2 or url_regex.search(token):\n",
    "                continue\n",
    "            elif token in stopwords:\n",
    "                continue\n",
    "            elif all(char in punctuations for char in token):\n",
    "                continue\n",
    "            elif token[0] == '@':\n",
    "                token = '@person' #Replace all friend tags with a common token.\n",
    "                tag = 'NNP'\n",
    "            word = lemmatizeToken(token, tag) if lemmatize else token\n",
    "            onegrams[word] += 1\n",
    "            lonegram.add(word)\n",
    "            #add word location to onegram dictionary\n",
    "            if word != \"nicotine\":\n",
    "                if word not in onegram_dict:\n",
    "                    onegram_dict[word] = [i]\n",
    "                elif i != onegram_dict[word][-1]:\n",
    "                    onegram_dict[word] += [i]\n",
    "            if prev is not None:\n",
    "                bigram = '-'.join([prev, word])\n",
    "                bigrams[bigram] += 1\n",
    "                lbigram.add(bigram)\n",
    "                #add bigram location to bigram dictionary\n",
    "                if bigram not in bigram_dict:\n",
    "                    bigram_dict[bigram] = [i]\n",
    "                elif i != bigram_dict[bigram][-1]:\n",
    "                    bigram_dict[bigram] += [i]\n",
    "                \n",
    "            prev = word\n",
    "        loc_onegrams.append(lonegram)\n",
    "        loc_bigrams.append(lbigram)\n",
    "        \n",
    "    return onegrams,bigrams, onegram_dict, bigram_dict, loc_onegrams, loc_bigrams\n",
    "\n",
    "def load_data(file_name, non_english=True):\n",
    "    '''\n",
    "        opens csv and runs normalize_tweets and generates onegrams and bigrams.\n",
    "    '''\n",
    "    df = pd.read_csv(file_name)\n",
    "    if(non_english):\n",
    "        df = remove_nonenglish(df)\n",
    "    tweets = list(df['text'])\n",
    "    print(\"Tweets count: \",len(tweets))\n",
    "    return(normalize_tweets(tweets,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above function, we remove non-english tweets and also generate bigrams and onegrams for english tweets. By removing non-english tweets, we are left with 371642 tweets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tweets count:  371642\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████| 371642/371642 [09:17<00:00, 667.17it/s]\n"
     ]
    }
   ],
   "source": [
    "onegrams, bigrams, onegram_dict, bigram_dict, loc_onegram, loc_bigram = load_data(\"english.csv\", non_english=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tweets containing the phrases \"bad nicotine\" (\"worse than nicotine\" is normalised to \"bad nicotine\"), \"nicotine herion\", and \"nicotine stain\" used in conjunction with \"silver spoons\" are references to song lyrics (Nicotine by Panic at the Disco, Never Be The Same by Camila Cabello, Nobody Home by Pink Floyd) and are not related to nicotine products. These tweets are removed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('english.csv')\n",
    "text = df['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def findWholeWord(w):\n",
    "    return re.compile(r'\\b({0})\\b'.format(w), flags=re.IGNORECASE).search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_index = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "68\n"
     ]
    }
   ],
   "source": [
    "nicotine_stain_silver = []\n",
    "count = 0\n",
    "for i in bigram_dict['nicotine-stain']:\n",
    "    sent = text[i]\n",
    "    if(findWholeWord('silver spoon')(sent)):\n",
    "        remove_index.append(i)\n",
    "        count += 1\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3524\n"
     ]
    }
   ],
   "source": [
    "print(bigrams['bad-nicotine'])\n",
    "remove_index.extend(bigram_dict['bad-nicotine'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2744\n"
     ]
    }
   ],
   "source": [
    "print(bigrams['nicotine-heroin'])\n",
    "remove_index.extend(bigram_dict['nicotine-heroin'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "939\n"
     ]
    }
   ],
   "source": [
    "print(bigrams['nicotine-heroine'])\n",
    "remove_index.extend(bigram_dict['nicotine-heroine']) # misspelling of nicotine herion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7212\n"
     ]
    }
   ],
   "source": [
    "remove_index = np.unique(remove_index)\n",
    "print(len(remove_index))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7212 tweets are removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(remove_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(364430, 4)\n"
     ]
    }
   ],
   "source": [
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 364430 tweets left. We now have to separate bot and non-bot users."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "214514\n"
     ]
    }
   ],
   "source": [
    "users = df['userId']\n",
    "unique_users = users.unique()\n",
    "print(len(unique_users))\n",
    "userdf = pd.DataFrame(unique_users)\n",
    "userdf.to_csv('users.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 214514 unique users"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The list of users generated above is used by the bot detection script that can be found as **bot_script.py** in the repository. \n",
    "**Note:** The script takes atleast a week to fully cover all users "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are several users who have deleted their profiles on Twitter. We cannot determine now if they were bots or not. We'll have to drop their posts. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('combined_users.pickle', 'rb') as file:\n",
    "    user_scores = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████| 214514/214514 [00:00<00:00, 673494.10it/s]\n"
     ]
    }
   ],
   "source": [
    "existing_users = []\n",
    "count = 0\n",
    "for user in tqdm.tqdm(unique_users):\n",
    "    if user_scores[user] != None:\n",
    "        existing_users.append(user)\n",
    "    else:\n",
    "        count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of deleted users: 27186\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of deleted users: \" + str(count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "existing_set = set(existing_users)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "364430it [00:01, 224932.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of posts removed: 42890\n"
     ]
    }
   ],
   "source": [
    "# remove posts belonging to deleted users\n",
    "existing_df_temp = []\n",
    "count = 0\n",
    "for row in tqdm.tqdm(df.itertuples()):\n",
    "    if row.userId in existing_set:\n",
    "        existing_df_temp += [row]\n",
    "    else:\n",
    "        count += 1\n",
    "\n",
    "print(\"Number of posts removed: \" + str(count))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "42890 posts are removed because of deleted users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "364430it [00:01, 197139.45it/s]\n"
     ]
    }
   ],
   "source": [
    "bot_df = []\n",
    "real_df = []\n",
    "bot_count = 0\n",
    "for row in tqdm.tqdm(df.itertuples()):\n",
    "    user = row.userId\n",
    "    if user_scores[user] == None:\n",
    "        continue;\n",
    "    elif user_scores[user]['display_scores']['english'] >= 4:\n",
    "        bot_count+=1\n",
    "        bot_df += [row]\n",
    "    else:\n",
    "        real_df += [row]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "real_df = pd.DataFrame(real_df)\n",
    "bot_df = pd.DataFrame(bot_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "300360\n"
     ]
    }
   ],
   "source": [
    "print(real_df.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The number of non-bot tweets are 300360"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21180\n"
     ]
    }
   ],
   "source": [
    "print(bot_df.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The number of bot tweets are 21180"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "181439"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(real_df.userId.unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The number of non-bot users is 181439 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5889"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(bot_df.userId.unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The number of non-bot users is 5889"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bot_df.to_csv('bots.csv', index=False)\n",
    "real_df.to_csv('non_bots.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cleanup process is complete and the above two dataframes are used in the analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
